{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "Itn9MdOhLVAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DO6F02OjsA51"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import time\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Some functions"
      ],
      "metadata": {
        "id": "4GL2PsOf-zgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_html_from_page(url):\n",
        "    '''\n",
        "    Returns a BeutifulSoup object given an URL in string format.\n",
        "    \n",
        "    :param url: string\n",
        "    \n",
        "    :return: BeautifulSoup object\n",
        "    '''\n",
        "    response = urlopen(url)\n",
        "    html = response.read()\n",
        "\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    return soup"
      ],
      "metadata": {
        "id": "s0laFAiYtG8M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_trash_list(raw_urls):\n",
        "    '''\n",
        "    Filter `raw_urls` and returns a list of URL in string format that will be descarted.\n",
        "    Only pages with `/news/` will pass.\n",
        "    \n",
        "    :param raw_urls: list\n",
        "    \n",
        "    :return: list\n",
        "    '''\n",
        "    trash_list = []\n",
        "\n",
        "    for item in raw_urls:\n",
        "        if item.find('/news/') == -1:\n",
        "            trash_list.append(item)\n",
        "\n",
        "    return trash_list"
      ],
      "metadata": {
        "id": "mav4lPlXzD-L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_items(raw_urls, trash_list):\n",
        "    '''\n",
        "    Removes from list `raw_urls` the elements of list `trash_list`.\n",
        "    \n",
        "    :param raw_urls: list\n",
        "    :param trash_list: list\n",
        "    \n",
        "    :return: list\n",
        "    '''\n",
        "    urls = raw_urls.copy()\n",
        "\n",
        "    for trash in trash_list:\n",
        "        urls.remove(trash)\n",
        "\n",
        "    return urls"
      ],
      "metadata": {
        "id": "NqBffa5n8zFM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crawling the main page to take the URL of all articles"
      ],
      "metadata": {
        "id": "Aw9UGt2xLaEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "site_url = 'https://www.bbc.com'\n",
        "\n",
        "# Getting the HTML from main page\n",
        "soup = get_html_from_page(site_url)\n",
        "\n",
        "raw_urls = []\n",
        "\n",
        "# Fetch the tag where the links are\n",
        "bs_news = soup.findAll('h3', {'class': 'media__title'})\n",
        "\n",
        "for item in bs_news:\n",
        "    if item.a.get('href')[0] == '/':\n",
        "        raw_urls.append(site_url + item.a.get('href'))\n",
        "    else:\n",
        "        raw_urls.append(item.a.get('href'))"
      ],
      "metadata": {
        "id": "7zeNlfMJw3Sr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering only URL that contain `/news/`\n",
        "trash_list = find_trash_list(raw_urls)\n",
        "urls = remove_items(raw_urls, trash_list)"
      ],
      "metadata": {
        "id": "og3sopQtxsWe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crawling all pages selected to fetch:\n",
        "- Headlines\n",
        "- Authors\n",
        "- Publication dates and times\n",
        "- Article texts\n",
        "- URL after the redirect"
      ],
      "metadata": {
        "id": "xbkksaCTJHKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "redirected_urls = []\n",
        "headlines = []\n",
        "authors = []\n",
        "publications_date_time = []\n",
        "article_texts = []\n",
        "\n",
        "for i, url in enumerate(urls):\n",
        "    \n",
        "    # Getting the HTML from each page\n",
        "    soup_page = get_html_from_page(url)\n",
        "    time.sleep(1)\n",
        "\n",
        "###############################################################################\n",
        "    # URLs redirected\n",
        "    redirected = soup_page.find('head').find('meta', {'property': 'og:url'}).get('content')\n",
        "    redirected_urls.append(redirected)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "     # Headlines\n",
        "    title = soup_page.find('article').h1.get_text()\n",
        "    headlines.append(title)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "    # Authors\n",
        "    authors_names = []\n",
        "    final_name = 'Author not identified'\n",
        "\n",
        "    try:\n",
        "        by_line_block = soup_page.find('article').find('div', {'data-component': 'byline-block'})\n",
        "        tag_div_author = by_line_block.find('div', {'class': 'ssrcss-68pt20-Text-TextContributorName e8mq1e96'})\n",
        "        complete_by_line = tag_div_author.get_text().split('By ')[1].split('&')\n",
        "        \n",
        "        # Sometimes there are more than one author\n",
        "        for n, author in enumerate(complete_by_line):\n",
        "            final_name = author.split(' and ')[0].split(' in ')[0].strip().split('BBC')[0]\n",
        "            authors_names.append(final_name)\n",
        "    \n",
        "    \n",
        "    # Some pages have no author on the front-end (pages with videos or photos only)\n",
        "    except:\n",
        "        # print(f'Redirected url without author\\n{redirected}', end='\\n\\n') # Only developer check\n",
        "        authors_names.append(final_name)\n",
        "    \n",
        "    authors.append(authors_names)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "    # Publication dates and times\n",
        "    try:\n",
        "        str_format = soup_page.findAll('time')[0].get('datetime')\n",
        "        \n",
        "        # Transforming the format\n",
        "        for char in ['.', '+']:\n",
        "            if str_format.find(char) != -1:\n",
        "                dt_format = datetime.strptime(str_format.split(char)[0], '%Y-%m-%dT%H:%M:%S')\n",
        "        publications_date_time.append(dt_format)\n",
        "\n",
        "    except:\n",
        "        print('Error in datetime')\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "    # Article texts\n",
        "    try:\n",
        "        paragraph = []\n",
        "        text_block = soup_page.findAll('div', {'data-component': 'text-block'})\n",
        "\n",
        "        # For pages with only videos, I get the video descriptions\n",
        "        if text_block == []:\n",
        "\n",
        "            tag_article = soup_page.find('article').find('div', {'data-testid': 'reveal-text-wrapper'})\n",
        "            paragraphs_list = tag_article.findAll('p')\n",
        "\n",
        "            video_description = []\n",
        "\n",
        "            # The video descriptions are separated in more than one tag `p`\n",
        "            for p in paragraphs_list:\n",
        "                \n",
        "                video_description.append(p.get_text().strip())\n",
        "            \n",
        "            # Merging all the texts in each tag and separating them with `\\n`\n",
        "            article_texts.append('\\n'.join(video_description))\n",
        "        \n",
        "        # For pages with text\n",
        "        else:\n",
        "\n",
        "            # The article text are separated in more than one tag `div`\n",
        "            for block in text_block:\n",
        "                paragraph.append(block.get_text().strip())\n",
        "                \n",
        "                try:\n",
        "                    # Some article texts have subheadings in the middle and without them the text could be meaningless\n",
        "                    if block.next_sibling['data-component'] == 'subheadline-block':\n",
        "                        paragraph.append(block.next_sibling.get_text())\n",
        "                except:\n",
        "                    pass\n",
        "            \n",
        "            # Merging all the texts in each tag and separating them with `\\n`\n",
        "            article_texts.append('\\n'.join(paragraph))\n",
        "    except:\n",
        "        print('Error in text')"
      ],
      "metadata": {
        "id": "anmu38Mnlywb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking how many items each list has"
      ],
      "metadata": {
        "id": "PTx3fhMBLN3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Urls ------------------------> {len(urls)}')\n",
        "print(f'Authors ---------------------> {len(authors)}')\n",
        "print(f'Headlines -------------------> {len(headlines)}')\n",
        "print(f'Articles texts --------------> {len(article_texts)}')\n",
        "print(f'Redirected urls -------------> {len(redirected_urls)}')\n",
        "print(f'Publications date and time --> {len(publications_date_time)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaxHmAwXP8O7",
        "outputId": "8499c0cd-b165-4d11-c656-804411498671"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Urls ------------------------> 36\n",
            "Authors ---------------------> 36\n",
            "Headlines -------------------> 36\n",
            "Articles texts --------------> 36\n",
            "Redirected urls -------------> 36\n",
            "Publications date and time --> 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing output"
      ],
      "metadata": {
        "id": "RL4Ee-4kMSKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict = {\n",
        "    'Headline': headlines,\n",
        "    'Authors': authors,\n",
        "    'Publication_datetime': publications_date_time,\n",
        "    'Article': article_texts,\n",
        "    \"URL\": urls,\n",
        "    'Redirected_URL': redirected_urls\n",
        "}\n",
        "\n",
        "data = pd.DataFrame(data_dict)\n",
        "\n",
        "for col in data.columns:\n",
        "    if col != 'Publication_datetime':\n",
        "        data[col] = data[col].astype(str)\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "Z_V3lLFcMS1W",
        "outputId": "df498ea1-9963-48ed-d7f3-fd1c2a596c3d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Headline  \\\n",
              "0  Ukraine names unarmed, smoking soldier shot by...   \n",
              "1  'Granby Girl' identified as Patricia Ann Tucke...   \n",
              "2  Kidnapped Americans were in Mexico for tummy tuck   \n",
              "3  Junk fees: US names and shames airlines over f...   \n",
              "4  Japan forced to destroy flagship H3 rocket in ...   \n",
              "\n",
              "                     Authors Publication_datetime  \\\n",
              "0  ['Author not identified']  2023-03-07 12:31:01   \n",
              "1  ['Author not identified']  2023-03-07 12:58:50   \n",
              "2  ['Author not identified']  2023-03-07 14:25:19   \n",
              "3              ['Chloe Kim']  2023-03-06 20:18:57   \n",
              "4            ['Joel Guinto']  2023-03-07 09:25:23   \n",
              "\n",
              "                                             Article  \\\n",
              "0  Ukraine has vowed to find the Russian soldiers...   \n",
              "1  A woman found shot to death in the state of Ma...   \n",
              "2  Four Americans kidnapped by heavily armed men ...   \n",
              "3  US airlines are being called out for their fam...   \n",
              "4  Japan was forced to blow up its new rocket dur...   \n",
              "\n",
              "                                                 URL  \\\n",
              "0     https://www.bbc.com/news/world-europe-64872623   \n",
              "1  https://www.bbc.com/news/world-us-canada-64874502   \n",
              "2  https://www.bbc.com/news/world-us-canada-64875131   \n",
              "3  https://www.bbc.com/news/world-us-canada-64870070   \n",
              "4       https://www.bbc.com/news/world-asia-64871603   \n",
              "\n",
              "                                      Redirected_URL  \n",
              "0     https://www.bbc.com/news/world-europe-64872623  \n",
              "1  https://www.bbc.com/news/world-us-canada-64874502  \n",
              "2  https://www.bbc.com/news/world-us-canada-64875131  \n",
              "3  https://www.bbc.com/news/world-us-canada-64870070  \n",
              "4       https://www.bbc.com/news/world-asia-64871603  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69273fe1-ee20-4af3-a504-7315e52d81f1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Publication_datetime</th>\n",
              "      <th>Article</th>\n",
              "      <th>URL</th>\n",
              "      <th>Redirected_URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ukraine names unarmed, smoking soldier shot by...</td>\n",
              "      <td>['Author not identified']</td>\n",
              "      <td>2023-03-07 12:31:01</td>\n",
              "      <td>Ukraine has vowed to find the Russian soldiers...</td>\n",
              "      <td>https://www.bbc.com/news/world-europe-64872623</td>\n",
              "      <td>https://www.bbc.com/news/world-europe-64872623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'Granby Girl' identified as Patricia Ann Tucke...</td>\n",
              "      <td>['Author not identified']</td>\n",
              "      <td>2023-03-07 12:58:50</td>\n",
              "      <td>A woman found shot to death in the state of Ma...</td>\n",
              "      <td>https://www.bbc.com/news/world-us-canada-64874502</td>\n",
              "      <td>https://www.bbc.com/news/world-us-canada-64874502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kidnapped Americans were in Mexico for tummy tuck</td>\n",
              "      <td>['Author not identified']</td>\n",
              "      <td>2023-03-07 14:25:19</td>\n",
              "      <td>Four Americans kidnapped by heavily armed men ...</td>\n",
              "      <td>https://www.bbc.com/news/world-us-canada-64875131</td>\n",
              "      <td>https://www.bbc.com/news/world-us-canada-64875131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Junk fees: US names and shames airlines over f...</td>\n",
              "      <td>['Chloe Kim']</td>\n",
              "      <td>2023-03-06 20:18:57</td>\n",
              "      <td>US airlines are being called out for their fam...</td>\n",
              "      <td>https://www.bbc.com/news/world-us-canada-64870070</td>\n",
              "      <td>https://www.bbc.com/news/world-us-canada-64870070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Japan forced to destroy flagship H3 rocket in ...</td>\n",
              "      <td>['Joel Guinto']</td>\n",
              "      <td>2023-03-07 09:25:23</td>\n",
              "      <td>Japan was forced to blow up its new rocket dur...</td>\n",
              "      <td>https://www.bbc.com/news/world-asia-64871603</td>\n",
              "      <td>https://www.bbc.com/news/world-asia-64871603</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69273fe1-ee20-4af3-a504-7315e52d81f1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-69273fe1-ee20-4af3-a504-7315e52d81f1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-69273fe1-ee20-4af3-a504-7315e52d81f1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload file to BigQuery"
      ],
      "metadata": {
        "id": "RGPAay2SxNvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_id = 'vini-project-379618'              \n",
        "dataset_id = 'coding_test_lima_consulting'\n",
        "table_id = 'bbc_news'\n",
        "\n",
        "destination_table = f'{dataset_id}.{table_id}'"
      ],
      "metadata": {
        "id": "Fu78zyYsAB4c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table_schema = [\n",
        "        {'name': 'Headline', 'type': 'STRING'},\n",
        "        {'name': 'Authors', 'type': 'STRING'},\n",
        "        {'name': 'Publication_datetime', 'type': 'DATETIME'},\n",
        "        {'name': 'Article', 'type': 'STRING'},\n",
        "        {'name': 'URL', 'type': 'STRING'},\n",
        "        {'name': 'Redirected_URL', 'type': 'STRING'}\n",
        "    ]"
      ],
      "metadata": {
        "id": "eqI_jbtjq-Xo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key_path = '/content/GBQ.json'\n",
        "\n",
        "scopes = [\n",
        "    'https://www.googleapis.com/auth/bigquery',\n",
        "    'https://www.googleapis.com/auth/cloud-platform'\n",
        "]\n",
        "\n",
        "credentials = service_account.Credentials.from_service_account_file(\n",
        "    key_path,\n",
        "    scopes=scopes\n",
        ")"
      ],
      "metadata": {
        "id": "gVFu5P7MxM2T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_gbq(\n",
        "    credentials=credentials,\n",
        "    destination_table=destination_table,\n",
        "    if_exists='replace',\n",
        "    project_id=project_id,\n",
        "    table_schema=table_schema\n",
        ")"
      ],
      "metadata": {
        "id": "3Gex6ZNC6sGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e451d26-f320-4d5a-d96b-a5dc97ba1b44"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 3858.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CsZC5ZjejFWl"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}